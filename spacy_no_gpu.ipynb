{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xMq18ab8Rt9q"
      },
      "outputs": [],
      "source": [
        "import locale\n",
        "locale.getpreferredencoding = lambda: \"UTF-8\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U7Uaoh2AF7RP",
        "outputId": "9e6e4622-990a-4653-81cb-dc7485bf71a3"
      },
      "outputs": [],
      "source": [
        "!pip install --no-deps spacy[cuda101]\n",
        "!pip install --no-deps thinc\n",
        "!python -m spacy download ru_core_news_md"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WUlaW0M3SDiu",
        "outputId": "908bf05e-7421-4534-87c5-ddbf06ad2083"
      },
      "outputs": [],
      "source": [
        "!pip install thinc-gpu-ops"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OpfFYWWzJOup",
        "outputId": "30736611-a450-47ce-bd53-72f513208b40"
      },
      "outputs": [],
      "source": [
        "!unzip /content/model_10_epochs.zip"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gt5fVv1SBdHo"
      },
      "outputs": [],
      "source": [
        "# @title Путь к датасету в формате json\n",
        "path = \"/content/ner_data_normalized.json\" # @param {type:\"string\"}\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hrZgaPbNJGbz"
      },
      "outputs": [],
      "source": [
        "import spacy\n",
        "from spacy.training.example import Example\n",
        "import json\n",
        "import random\n",
        "\n",
        "nlp = spacy.load('ru_core_news_md', exclude=['parser', 'ner'])\n",
        "ner = nlp.add_pipe('ner')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "KumXzFOiF7RR",
        "outputId": "2fd141be-3971-4ea2-b773-395530e3628a"
      },
      "outputs": [],
      "source": [
        "with open(path, \"r\", encoding=\"utf-8\") as f:\n",
        "    data = json.load(f)\n",
        "\n",
        "    # Iterate over the data and create training examples\n",
        "    entity_examples = []\n",
        "    for example in data[\"data\"]:\n",
        "        text = example[\"video_info\"]\n",
        "        entities = example[\"entities\"]\n",
        "\n",
        "        # Create a list of entities for the example\n",
        "        entity_spans = []\n",
        "        for entity in entities:\n",
        "            start = entity[\"offset\"]\n",
        "            end = start + entity[\"length\"]\n",
        "            label = entity[\"label\"]\n",
        "            entity_spans.append((start, end, label))\n",
        "\n",
        "        # Create a training example with text and entities\n",
        "        example = Example.from_dict(nlp.make_doc(text), {\"entities\": entity_spans})\n",
        "\n",
        "        entity_examples.append(example)\n",
        "\n",
        "# Add the examples to the entity recognizer\n",
        "ner.initialize(lambda: entity_examples)\n",
        "\n",
        "# Disable other pipeline components except NER during training\n",
        "pipe_exceptions = [\"ner\"]\n",
        "other_pipes = [pipe for pipe in nlp.pipe_names if pipe not in pipe_exceptions]\n",
        "\n",
        "with nlp.select_pipes(disable=other_pipes):\n",
        "    # Initialize the training loop\n",
        "    optimizer = nlp.initialize()\n",
        "\n",
        "    for iteration in range(30):\n",
        "        losses = {}\n",
        "        # Shuffle the training data before each iteration\n",
        "        random.shuffle(entity_examples)\n",
        "        for batch in spacy.util.minibatch(entity_examples, size=4):\n",
        "            nlp.update(batch, losses=losses, sgd=optimizer)\n",
        "\n",
        "        print(f\"Iteration {iteration+1}: Losses - {losses}\")\n",
        "\n",
        "# Save the trained model\n",
        "nlp.to_disk(\"ner_model\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ighM3h2MkvJB",
        "outputId": "04533547-1b16-4392-f058-cd786f82f4eb"
      },
      "outputs": [],
      "source": [
        "!zip -r /content/model_30_epochs.zip /content/ner_model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 647
        },
        "id": "Y_kZXN2fF7RT",
        "outputId": "69f75284-4fad-477c-93a4-07fb8e37fdf7"
      },
      "outputs": [],
      "source": [
        "nlp = spacy.load('/content/content/ner_model')\n",
        "video_url = \"https://rutube.ru/video/237559720b0b079f5fb621ef08092a59/\" # @param {type:\"string\"}\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "\n",
        "# Replace VIDEO_URL with the URL of the rutube video\n",
        "VIDEO_URL = video_url\n",
        "\n",
        "title = \"\"\n",
        "description = \"\"\n",
        "\n",
        "def parse_description():\n",
        "    # Send GET request to the video page\n",
        "    response = requests.get(VIDEO_URL)\n",
        "    if response.status_code == 200:\n",
        "        # Parse HTML content\n",
        "        soup = BeautifulSoup(response.content, \"html.parser\")\n",
        "        # Find the description element\n",
        "        description_element = soup.select_one(\".pen-videopage-description\")\n",
        "        title_element = soup.select_one(\".video-pageinfo-container-module__videoTitle\")\n",
        "        if description_element:\n",
        "            title = title_element.text.strip()\n",
        "            description = description_element.text.strip()\n",
        "            return(title + description)\n",
        "        else:\n",
        "            print(\"Description not found\")\n",
        "    else:\n",
        "        print(\"Failed to retrieve video page\")\n",
        "\n",
        "\n",
        "doc = nlp(parse_description())\n",
        "doc.ents\n",
        "\n",
        "from spacy import displacy\n",
        "displacy.render(doc, style=\"ent\", jupyter=True)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NC5YyH1-1P97",
        "outputId": "4f7d8de9-6e81-4f43-a45d-3745f602582d"
      },
      "outputs": [],
      "source": [
        "def form_ents(ents, text):\n",
        "  return [{'label':i['label'], 'offset':i['start'], 'length':i['end']-i['start'], 'segment':text[i['start']:i['end']]} for i in ents]\n",
        "\n",
        "def text_to_notm(text):\n",
        "  doc = nlp(text)\n",
        "  return [text, form_ents(doc.to_json()['ents'], text)]\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OPf3-w7q4BeC"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import csv\n",
        "import json\n",
        "pd.options.display.max_colwidth = 300\n",
        "path = '/content/ner_data_test.csv' # @param {type:\"string\"}\n",
        "\n",
        "def csv_to_csv(path):\n",
        "  with open('new_ner_data_test.csv', 'w+', newline='', encoding='utf-8') as file_csv:\n",
        "    writer = csv.writer(file_csv)\n",
        "    writer.writerows([[\"video_info\", \"entities_prediction\"]])\n",
        "    data = pd.read_csv(path)\n",
        "    for index, row in data.iterrows():\n",
        "      text = row['video_info']\n",
        "      res = text_to_notm(text)\n",
        "      result = res[1]\n",
        "      result = json.dumps(result, ensure_ascii=False)\n",
        "      result = result.replace('\\\"', '\\\"')\n",
        "      result = result.replace(',', '\\,')\n",
        "      result = result.replace('}\\, {', '}, {')\n",
        "      result = result[1:-1]\n",
        "      writer.writerows([[res[0], result]])\n",
        "    file_csv.close()\n",
        "\n",
        "\n",
        "\n",
        "csv_to_csv(path)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
